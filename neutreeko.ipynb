{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0d7ad9e755df8099726d839c3e994e03326c5e1882c6be7d8bb03ddd7a6d9257d",
   "display_name": "Python 3.8.8 64-bit ('feup-iart-proj2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Reinforced Learning over implified Neutreeko\n",
    "\n",
    "This work consist of creating an agent capable of playing a simplified version of the game Neutreeko (https://www.neutreeko.net/neutreeko.htm).\n",
    "\n",
    "![alt text](https://i.imgur.com/qeEH8e2.jpg)\n",
    "\n",
    "In the simplified version, the game starts with a randomly generated 5x5 board with 3 black pieces. The Agent can move any piece in one of the 4 possible directions: UP, DOWN, LEFT, RIGHT. After choosing a direction and a piece, the selected piece will move in that direction until colliding with a edge of the board or another piece.\n",
    "\n",
    "The game ends when 200 rounds pass or when the player is able to place 3 pieces together in a row, column or diagonally.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Necessary imports\n",
    "- gym\n",
    "- numpy\n",
    "- matplotlib\n",
    "- typing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Union"
   ]
  },
  {
   "source": [
    "## Game Logic and Engine\n",
    "\n",
    "For better organization, we separated some logic of the game into the class `NeutreekoUtils` and created some universal constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EASY_ACTIONS_DICT = {\n",
    "    'UP': (-1, 0),\n",
    "    'DOWN': (+1, 0),\n",
    "    'LEFT': (0, -1),\n",
    "    'RIGHT': (0, +1),\n",
    "}\n",
    "\n",
    "BOARD_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeutreekoUtils:\n",
    "    @staticmethod\n",
    "    def search_sequence_numpy(arr, seq) -> bool:\n",
    "        \"\"\"\n",
    "        Find sequence in an array using NumPy only.\n",
    "        :param arr: input 1D array\n",
    "        :param seq: input 1D array\n",
    "        :return: True if the seq is in the arr\n",
    "        \"\"\"\n",
    "\n",
    "        # Store sizes of input array and sequence\n",
    "        Na, Nseq = arr.size, seq.size\n",
    "\n",
    "        # Range of sequence\n",
    "        r_seq = np.arange(Nseq)\n",
    "\n",
    "        # Create a 2D array of sliding indices across the entire length of input array.\n",
    "        # Match up with the input sequence & get the matching starting indices.\n",
    "        M = (arr[np.arange(Na-Nseq+1)[:, None] + r_seq] == seq).all(1)\n",
    "\n",
    "        # Return true if the sequence exists\n",
    "        return M.any() > 0\n",
    "\n",
    "    @staticmethod\n",
    "    def find_sequence_board(board: np.array, sequence) -> bool:\n",
    "        \"\"\"\n",
    "        Given a board, attempts to find a sequence in all possible directions\n",
    "        :param board:\n",
    "        :param sequence:\n",
    "        :return: True if the sequence is in the board, False otherwise\n",
    "        \"\"\"\n",
    "        for i in range(len(board)):\n",
    "            # Check in lines\n",
    "            if NeutreekoUtils.search_sequence_numpy(board[i, :], sequence):\n",
    "                return True\n",
    "            # check in columns\n",
    "            if NeutreekoUtils.search_sequence_numpy(board[:, i], sequence):\n",
    "                return True\n",
    "\n",
    "        # check victory in diagonals\n",
    "        flipped_board = np.fliplr(board)\n",
    "        for i in range(-2, 3):\n",
    "            diagonal1 = np.diagonal(board, offset=i)\n",
    "            if NeutreekoUtils.search_sequence_numpy(diagonal1, sequence):\n",
    "                return True\n",
    "            diagonal2 = np.diagonal(flipped_board, offset=i)\n",
    "            if NeutreekoUtils.search_sequence_numpy(diagonal2, sequence):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def value_in_board(board, coords: Tuple[int, int]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the value in the board\n",
    "\n",
    "        :param board: A np array of size (5,5)\n",
    "        :param coords: The x and y coordinates of a spot\n",
    "        :return: The integer value in the board\n",
    "        \"\"\"\n",
    "        if (coords[0] < 0) | (coords[0] >= BOARD_SIZE) | (coords[1] < 0) | (coords[1] >= BOARD_SIZE):\n",
    "            return False\n",
    "        return board[coords[0], coords[1]]\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_in_board(board, coords: Tuple[int, int], value: int) -> Union[None, bool]:\n",
    "        \"\"\"\n",
    "        Replaces a value in the board\n",
    "\n",
    "        :param board: A np array of size (5,5)\n",
    "        :param coords: The x and y coordinates of a spot\n",
    "        :param value: the value to be in the board\n",
    "        :return: None or False if the coords are not valid\n",
    "        \"\"\"\n",
    "        if (coords[0] < 0) | (coords[0] >= BOARD_SIZE) | (coords[1] < 0) | (coords[1] >= BOARD_SIZE):\n",
    "            return False\n",
    "        board[coords[0], coords[1]] = value"
   ]
  },
  {
   "source": [
    "### Game Logic \n",
    "\n",
    "For the game logic, we created the class `NeutreekoEasyGame` where we can create a new random board, check if a move is valid, retrieve a list of all possible moves, perform a move and update the board."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeutreekoEasyGame:\n",
    "    def __init__(self):\n",
    "        self.board = None\n",
    "        self.current_player = None\n",
    "        self.game_over = None\n",
    "        self.turns_count = None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Resets the game, with a new board, turns count to 0 and designates the first player\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.board = self.new_board()\n",
    "        self.current_player = 1\n",
    "        self.game_over = False\n",
    "        self.turns_count = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def new_board() -> np.array:\n",
    "        \"\"\"\n",
    "        Returns a random starting board, each element is a numpy.int8 (-128, 127)\n",
    "\n",
    "        :return: numpy.array\n",
    "        \"\"\"\n",
    "        board = np.array([[0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0]], dtype=np.int8)\n",
    "        piece_1_x = np.random.randint(0, 5)\n",
    "        piece_1_y = np.random.randint(0, 5)\n",
    "\n",
    "        piece_2_x = np.random.randint(0, 5)\n",
    "        piece_2_y = np.random.randint(0, 5)\n",
    "        while (piece_1_x == piece_2_x) & (piece_1_y == piece_2_y):\n",
    "            piece_2_x = np.random.randint(0, 5)\n",
    "            piece_2_y = np.random.randint(0, 5)\n",
    "\n",
    "        piece_3_x = np.random.randint(0, 5)\n",
    "        piece_3_y = np.random.randint(0, 5)\n",
    "        while ((piece_1_x == piece_3_x) & (piece_1_y == piece_3_y)) | ((piece_2_x == piece_3_x) & (piece_2_y == piece_3_y)):\n",
    "            piece_3_x = np.random.randint(0, 5)\n",
    "            piece_3_y = np.random.randint(0, 5)\n",
    "\n",
    "        board[piece_1_x][piece_1_y] = 1\n",
    "        board[piece_2_x][piece_2_y] = 1\n",
    "        board[piece_3_x][piece_3_y] = 1\n",
    "\n",
    "        return board\n",
    "\n",
    "    def value_in_board(self, position: Tuple[int, int]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the value in a position of the board\n",
    "\n",
    "        :param position: Tuple with 2 ints representing the coordinates of a cell\n",
    "        :return: The int value\n",
    "        \"\"\"\n",
    "        return NeutreekoUtils.value_in_board(self.board, position)\n",
    "\n",
    "    def replace_in_board(self, position: Tuple[int, int], value: int) -> None:\n",
    "        \"\"\"\n",
    "        Replaces a value in a board position\n",
    "\n",
    "        :param position: Tuple with 2 ints representing the coordinates of a cell\n",
    "        :param value: Value of a player\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        NeutreekoUtils.replace_in_board(self.board, position, value)\n",
    "\n",
    "    def free_cell(self, coords: Tuple[int, int]) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a cell is within bounds of the board and if it is free (value is 0)\n",
    "\n",
    "        :param coords: Tuple with 2 ints representing the coordinates of a cell\n",
    "        :return: True if the cell equals 0 and is within bounds\n",
    "        \"\"\"\n",
    "        if (coords[0] < 0) | (coords[0] >= BOARD_SIZE) | (coords[1] < 0) | (coords[1] >= BOARD_SIZE):\n",
    "            return False\n",
    "        value = self.value_in_board(coords)\n",
    "        return value == 0\n",
    "\n",
    "    def check_direction(self, coords: Tuple[int, int], direction: str) -> Union[None, Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Returns the resulting position given a starting position and a direction.\n",
    "        If the direction is not valid (can't make progress in that direction), returns None\n",
    "\n",
    "        :param coords: Coordinates of intial point\n",
    "        :param direction: String representation of the direction to take\n",
    "        :return: None if direction is not valid OR tuple with new coords of resulting positions\n",
    "        \"\"\"\n",
    "        action_coords = EASY_ACTIONS_DICT[direction]\n",
    "        attempt_coords = tuple(np.add(coords, action_coords))\n",
    "        free_cell = self.free_cell(attempt_coords)\n",
    "        if not free_cell:\n",
    "            return None\n",
    "        # apply direction until it reaches EOB (end of board) or another piece\n",
    "        while free_cell:\n",
    "            new_coords = attempt_coords\n",
    "            attempt_coords = tuple(np.add(new_coords, action_coords))\n",
    "            free_cell = self.free_cell(attempt_coords)\n",
    "        return new_coords\n",
    "\n",
    "    def available_directions(self, coords: Tuple[int, int]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Finds which directions are available for a piece on the coords tuple\n",
    "\n",
    "        :param coords: The coordinates of a piece\n",
    "        :return: A list of directions\n",
    "        \"\"\"\n",
    "        dirs = []\n",
    "        for action_name in EASY_ACTIONS_DICT.keys():\n",
    "            result = self.check_direction(coords, action_name)\n",
    "            if result:\n",
    "                dirs.append(action_name)\n",
    "        return dirs\n",
    "\n",
    "    def get_possible_moves(self, player: int, only_valid: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        Return all the possible moves for a given player with the current board\n",
    "\n",
    "        :param player: Integer representing the player\n",
    "        :param only_valid: returns only the valid moves\n",
    "        :return: A list of ints representing possible actions\n",
    "        \"\"\"\n",
    "        dirs_value = {\n",
    "            'UP': 0,\n",
    "            'DOWN': 1,\n",
    "            'LEFT': 2,\n",
    "            'RIGHT': 3\n",
    "        }\n",
    "        possible_moves = []\n",
    "\n",
    "        # Find player piece positions\n",
    "        result = np.where(self.board == player)\n",
    "        list_of_coordinates = list(zip(result[0], result[1]))\n",
    "\n",
    "        # for each player piece\n",
    "        piece_value = 0\n",
    "        for pos in list_of_coordinates:\n",
    "            if only_valid:\n",
    "                # checks which directions are available\n",
    "                dirs = self.available_directions(pos)\n",
    "                for dir in dirs:\n",
    "                    value = 4*piece_value + dirs_value[dir]\n",
    "                    possible_moves.append(value)\n",
    "            else:\n",
    "                # adds every direction\n",
    "                for dir in EASY_ACTIONS_DICT.keys():\n",
    "                    possible_moves.append(4*piece_value + dirs_value[dir])\n",
    "            piece_value += 1\n",
    "        return possible_moves\n",
    "\n",
    "    def action_handler(self, pos, dir) -> Union[None, Tuple[tuple, str]]:\n",
    "        \"\"\"\n",
    "        Effectuates the movement of the piece in pos, in the direction dir\n",
    "\n",
    "        :param pos: The position of the piece that will be moved\n",
    "        :param dir: The direction that the piece will be moved to\n",
    "        :return: A tuple with the resulting position and the move type. None if the move is not valid\n",
    "        \"\"\"\n",
    "        result = self.check_direction(pos, dir)\n",
    "        if not result:\n",
    "            return None\n",
    "\n",
    "        self.update_game(pos, result)\n",
    "        self.turns_count += 1\n",
    "\n",
    "        self.game_over = NeutreekoUtils.find_sequence_board(self.board, np.array([1, 1, 1]))\n",
    "\n",
    "        move_type = \"win\" if self.game_over else \"default\"\n",
    "        return result, move_type\n",
    "\n",
    "    def update_game(self, pos, result) -> None:\n",
    "        \"\"\"\n",
    "        Replaces the piece in the board\n",
    "        :param pos: initial position of the piece\n",
    "        :param result: final position of the piece\n",
    "        \"\"\"\n",
    "        self.replace_in_board(result, 1)\n",
    "        self.replace_in_board(pos, 0)\n",
    "        pass\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"\n",
    "        Renders the game on the screen\n",
    "        \"\"\"\n",
    "        print(self.board)"
   ]
  },
  {
   "source": [
    "## Environments\n",
    "\n",
    "After having the game engine correctly working, we can implement the simplified Neutreeko environment as the `NeutreekoEasyEnv` class with the methods `init`, `step`, `render`, `close` and `reset`. The Environment also needs a helper class `Reward` that holds the reward for each type of move."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward:\n",
    "    @staticmethod\n",
    "    def get(move_type):\n",
    "        \"\"\"\n",
    "        Chooses a reward value based on the type of action\n",
    "        :param move_type: type of action\n",
    "        :return: the reward value\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"win\": 20,  # winning move\n",
    "            # \"2_row\": 5,  # places 2 pieces together\n",
    "            # \"between\": 2,  # gets in between 2 opponent pieces\n",
    "            \"default\": -1  # makes a move (negative to not enforce unnecessary moves)\n",
    "        }.get(move_type, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeutreekoEasyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "       In a 5x5 board there are 3 black pieces in a random formation\n",
    "       The black pieces can move in any direction, they can move until\n",
    "       they collide with another piece or the edge of the board\n",
    "       The game ends when the 3 pieces make a line in any direction\n",
    "    Source:\n",
    "       This environment corresponds to a simplified version of the Neutreeko game\n",
    "       Specified here: https://www.neutreeko.net/neutreeko.htm\n",
    "    Observation:\n",
    "       Type: Discrete(2300)\n",
    "\n",
    "        [[0, 1, 0, 1, 0],\n",
    "         [0, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0],\n",
    "         [0, 0, 1, 0, 0],\n",
    "         [0, 0, 0, 0, 0]]\n",
    "\n",
    "       All possible board combinations\n",
    "    Actions:\n",
    "       Type: Discrete(12)\n",
    "\n",
    "       Num   Action\n",
    "       0      0-UP\n",
    "       1      0-DOWN\n",
    "       2      0-LEFT\n",
    "       3      0-RIGHT\n",
    "       4      1-UP\n",
    "       5      1-DOWN\n",
    "       6      1-LEFT\n",
    "       7      1-RIGHT\n",
    "       8      2-UP\n",
    "       9      2-DOWN\n",
    "       10     2-LEFT\n",
    "       11     2-RIGHT\n",
    "\n",
    "          UP   DOWN  LEFT  RIGHT\n",
    "       0  0     1     2     3\n",
    "       1  4     5     6     7\n",
    "       2  8     9     10    11\n",
    "\n",
    "       The piece 0 is the one with the lowest index. For a piece in coords (x, y), its index is 5*x + y.\n",
    "    Reward:\n",
    "       Reward class\n",
    "    Starting State:\n",
    "        A randomly generated board\n",
    "    Episode Termination:\n",
    "       The player makes 3 in a row\n",
    "       Episode length is greater than 200.\n",
    "   \"\"\"\n",
    "    metadata = {\n",
    "        'render.modes': ['terminal']\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode='terminal', max_turns=200):\n",
    "        super(NeutreekoEasyEnv, self).__init__()\n",
    "\n",
    "        # 3 pieces and 4 directions possible\n",
    "        self.action_space = gym.spaces.Discrete(3*4)\n",
    "        self.observation_space = gym.spaces.Discrete(2300)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.max_turns = max_turns\n",
    "\n",
    "        self.game = NeutreekoEasyGame()\n",
    "        pass\n",
    "\n",
    "    def step(self, action: int) -> Tuple[object, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Performs an action on the game and returns info\n",
    "        :param action:\n",
    "        :return: observation, reward, done, info\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        info = {\n",
    "            'old_state': np.copy(self.game.board),\n",
    "            'turn': self.game.turns_count,\n",
    "            'action': action,\n",
    "            'direction': None,\n",
    "        }\n",
    "\n",
    "        assert not self.done\n",
    "        # if self.done:\n",
    "        #     logger.warn(\"You are calling 'step()' even though this environment has already returned done = True.\"\n",
    "        #                 \"You should always call 'reset()' once you receive 'done = True'\"\n",
    "        #                 \"-- any further steps are undefined behavior.\")\n",
    "        pos, dir = self.process(action)\n",
    "        move_check = self.game.action_handler(pos, dir)\n",
    "        if move_check:\n",
    "            new_pos, move_type = move_check\n",
    "            reward = Reward.get(move_type)\n",
    "            info['direction'] = dir\n",
    "            info['new_position'] = new_pos\n",
    "\n",
    "        return self.observation, reward, self.done, info\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Resets the game\n",
    "        \"\"\"\n",
    "        self.game.reset()\n",
    "\n",
    "    def render(self, mode='terminal') -> None:\n",
    "        \"\"\"\n",
    "        Renders the game according to the mode\n",
    "        :param mode: terminal or GUI\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if mode == 'terminal':\n",
    "            self.game.render()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the environment and terminates anything if necessary\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def done(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the game is done or the max turns were reached\n",
    "        :return: True if the game is done, false otherwise\n",
    "        \"\"\"\n",
    "        game_over = self.game.game_over\n",
    "        too_many_turns = (self.game.turns_count > self.max_turns)\n",
    "        return game_over or too_many_turns\n",
    "\n",
    "    @property\n",
    "    def observation(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns the game board\n",
    "        :return: The board as a numpy array\n",
    "        \"\"\"\n",
    "        return np.copy(self.game.board)\n",
    "\n",
    "    def process(self, action: int) -> Tuple[tuple, str]:\n",
    "        \"\"\"\n",
    "        Convert a action into a position and direction\n",
    "        :param action: A integer between 0 and 11 representing an action\n",
    "        :return: A tuple with a position (tuple) and a direction\n",
    "        \"\"\"\n",
    "        directions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "        result = np.where(self.game.board == 1)\n",
    "        list_of_coordinates = list(zip(result[0], result[1]))\n",
    "        return list_of_coordinates[action // 4], directions[action % 4]\n"
   ]
  },
  {
   "source": [
    "## Agent Analysis\n",
    "### Random Agent\n",
    "\n",
    "To test if the environment and engine are working correctly, we can use a Agent that chooses random moves. The unused methods will be used for other agents"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def choice(self, env) -> int:\n",
    "        \"\"\"\n",
    "        Given the environment, choose the action to take using randomness\n",
    "        :param env: A Game environment\n",
    "        :return: the action to take\n",
    "        \"\"\"\n",
    "        player = env.game.current_player\n",
    "        possible_moves = env.game.get_possible_moves(player, only_valid=True)\n",
    "        i_random = np.random.randint(len(possible_moves))\n",
    "        return possible_moves[i_random]\n",
    "\n",
    "    def update(self, obs, reward, done, info, env):\n",
    "        \"\"\"\n",
    "        Not used in this agent\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def episode_update(self, episode):\n",
    "        \"\"\"\n",
    "        Not used in this agent\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "source": [
    "With the Agent implemented, we can test run the agent, environment and engine with a game loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = NeutreekoEasyEnv(render_mode='terminal')\n",
    "agent = RandomAgent()\n",
    "\n",
    "# Creating lists to keep track of reward and epsilon values\n",
    "training_rewards = []\n",
    "\n",
    "NB_EPISODES = 1000\n",
    "for episode in range(1, NB_EPISODES + 1):\n",
    "    # Resetting the environment each time as per requirement\n",
    "    env.reset()\n",
    "    # Starting the tracker for the rewards\n",
    "    total_training_rewards = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choice(env)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        agent.update(obs, reward, done, info, env)\n",
    "        total_training_rewards += reward\n",
    "    print(f\"Episode {episode: <4} finished after {env.game.turns_count} turns\")\n",
    "\n",
    "    agent.episode_update(episode)\n",
    "\n",
    "    # Adding the total reward\n",
    "    training_rewards.append(total_training_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results and total reward over all episodes\n",
    "x = range(NB_EPISODES)\n",
    "plt.plot(x, training_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Training total reward')\n",
    "plt.title('Total rewards over all episodes in training')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "With the Random Agent there isn't any kind of learning and the total reward for each episode is mostly negative.\n",
    "\n",
    "### Q-Learning Agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, observation_space=2300, action_space=12):\n",
    "        \"\"\"\n",
    "        Initialize an agent and its parameters\n",
    "\n",
    "        :param observation_space: How many possible states there are\n",
    "        :param action_space: How many actions there are\n",
    "        \"\"\"\n",
    "        self.Q = np.zeros((observation_space, action_space))\n",
    "\n",
    "        self.alpha = 0.7  # learning rate\n",
    "        self.discount_factor = 0.618\n",
    "        self.epsilon = 1\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay = 0.001\n",
    "\n",
    "        self.board_dict = {}\n",
    "        self.lastID = None\n",
    "\n",
    "    def choice(self, env) -> int:\n",
    "        \"\"\"\n",
    "        Given the environment, choose the action to take\n",
    "\n",
    "        :param env: A Game environment\n",
    "        :return: the action to take\n",
    "        \"\"\"\n",
    "        # Choosing an action given the states based on a random number\n",
    "        exp_exp_tradeoff = np.random.uniform(0, 1)\n",
    "\n",
    "        if repr(env.observation) not in self.board_dict:\n",
    "            if self.lastID:\n",
    "                self.lastID += 1\n",
    "            else:\n",
    "                self.lastID = 0\n",
    "            self.board_dict[repr(env.observation)] = self.lastID\n",
    "            state = self.lastID\n",
    "        else:\n",
    "            state = self.board_dict[repr(env.observation)]\n",
    "        # STEP 2: FIRST option for choosing the initial action - exploit\n",
    "        # If the random number is larger than epsilon: employing exploitation\n",
    "        # and selecting best action\n",
    "        if exp_exp_tradeoff > self.epsilon:\n",
    "            action = np.argmax(self.Q[state, :])\n",
    "            # Sometimes the agent will try to exploit, but end up choosing a not valid move\n",
    "            # To ensure that exploiting always provides good results, there's a check to verify if the\n",
    "            # action is valid, if it is not, chooses a random valid move\n",
    "            possible_moves = env.game.get_possible_moves(1, only_valid=True)\n",
    "            if action not in possible_moves:\n",
    "                i_random = np.random.randint(len(possible_moves))\n",
    "                action = possible_moves[i_random]\n",
    "\n",
    "        # STEP 2: SECOND option for choosing the initial action - explore\n",
    "        # Otherwise, employing exploration: choosing a random action\n",
    "        else:\n",
    "            possible_moves = env.game.get_possible_moves(1, only_valid=True)\n",
    "            i_random = np.random.randint(len(possible_moves))\n",
    "            action = possible_moves[i_random]\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self, obs, reward: int, done: bool, info: dict, env) -> None:\n",
    "        \"\"\"\n",
    "        Updates the Q-table after performing an action\n",
    "\n",
    "        :param obs: New state that resulted from a action\n",
    "        :param reward: The reward returned from applying a action to a state\n",
    "        :param done: Boolean representing if the episode is finished\n",
    "        :param info: Additional info from performing an action\n",
    "        :param env: The environment\n",
    "        \"\"\"\n",
    "        if repr(obs) not in self.board_dict:\n",
    "            if not self.board_dict:\n",
    "                self.board_dict[repr(obs)] = 0\n",
    "                self.lastID = 0\n",
    "            else:\n",
    "                self.lastID += 1\n",
    "                self.board_dict[repr(obs)] = self.lastID\n",
    "\n",
    "        state = self.board_dict[repr(info[\"old_state\"])]\n",
    "        new_state = self.board_dict[repr(obs)]\n",
    "        action = info['action']\n",
    "\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * (reward + self.discount_factor * np.max(self.Q[new_state, :]) - self.Q[state, action])\n",
    "\n",
    "    def episode_update(self, episode: int) -> None:\n",
    "        \"\"\"\n",
    "        Update internals after each episode\n",
    "\n",
    "        :param episode: Finished episode id\n",
    "        \"\"\"\n",
    "        # Cutting down on exploration by reducing the epsilon\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay*episode)"
   ]
  },
  {
   "source": [
    "With the new agent, now we try the game loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "env = NeutreekoEasyEnv(render_mode='terminal')\n",
    "agent = QAgent()\n",
    "\n",
    "# Creating lists to keep track of reward and epsilon values\n",
    "training_rewards = []\n",
    "epsilons = []\n",
    "\n",
    "NB_EPISODES = 1000\n",
    "for episode in range(1, NB_EPISODES + 1):\n",
    "    # Resetting the environment each time as per requirement\n",
    "    env.reset()\n",
    "    # Starting the tracker for the rewards\n",
    "    total_training_rewards = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choice(env)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        agent.update(obs, reward, done, info, env)\n",
    "        total_training_rewards += reward\n",
    "    print(f\"Episode {episode: <4} finished after {env.game.turns_count} turns\")\n",
    "\n",
    "    agent.episode_update(episode)\n",
    "\n",
    "    # Adding the total reward and reduced epsilon values\n",
    "    training_rewards.append(total_training_rewards)\n",
    "    epsilons.append(agent.epsilon)\n",
    "    \n",
    "print(f'Highest board id -> {agent.lastID}')\n",
    "print(f'Q-table -> {agent.Q}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results and total reward over all episodes\n",
    "x = range(NB_EPISODES)\n",
    "plt.plot(x, training_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Training total reward')\n",
    "plt.title('Total rewards over all episodes in training')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the epsilons over all episodes\n",
    "plt.plot(epsilons)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title(\"Epsilon for episode\")\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "### SARSA Agent\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    def __init__(self, observation_space=2300, action_space=12):\n",
    "        \"\"\"\n",
    "        Initialize an agent and its parameters\n",
    "        :param observation_space: How many possible states there are\n",
    "        :param action_space: How many actions there are\n",
    "        \"\"\"\n",
    "        self.Q = np.zeros((observation_space, action_space))\n",
    "\n",
    "        self.alpha = 0.7  # learning rate\n",
    "        self.discount_factor = 0.618\n",
    "        self.epsilon = 1\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay = 0.005\n",
    "\n",
    "        self.board_dict = {}\n",
    "        self.lastID = None\n",
    "\n",
    "    def choice(self, env) -> int:\n",
    "        \"\"\"\n",
    "        Given the environment, choose the action to take\n",
    "        :param env: A Game environment\n",
    "        :return: the action to take\n",
    "        \"\"\"\n",
    "        # Choosing an action given the states based on a random number\n",
    "        exp_exp_tradeoff = np.random.uniform(0, 1)\n",
    "\n",
    "        if repr(env.observation) not in self.board_dict:\n",
    "            if self.lastID:\n",
    "                self.lastID += 1\n",
    "            else:\n",
    "                self.lastID = 0\n",
    "            self.board_dict[repr(env.observation)] = self.lastID\n",
    "            state = self.lastID\n",
    "        else:\n",
    "            state = self.board_dict[repr(env.observation)]\n",
    "        # STEP 2: FIRST option for choosing the initial action - exploit\n",
    "        # If the random number is larger than epsilon: employing exploitation\n",
    "        # and selecting best action\n",
    "        if exp_exp_tradeoff > self.epsilon:\n",
    "            action = np.argmax(self.Q[state, :])\n",
    "            # Sometimes the agent will try to exploit, but end up choosing a not valid move\n",
    "            # To ensure that exploiting always provides good results, there's a check to verify if the\n",
    "            # action is valid, if it is not, chooses a random valid move\n",
    "            possible_moves = env.game.get_possible_moves(1, only_valid=True)\n",
    "            if action not in possible_moves:\n",
    "                i_random = np.random.randint(len(possible_moves))\n",
    "                action = possible_moves[i_random]\n",
    "\n",
    "        # STEP 2: SECOND option for choosing the initial action - explore\n",
    "        # Otherwise, employing exploration: choosing a random action\n",
    "        else:\n",
    "            possible_moves = env.game.get_possible_moves(1, only_valid=True)\n",
    "            i_random = np.random.randint(len(possible_moves))\n",
    "            action = possible_moves[i_random]\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self, obs, reward: int, done: bool, info: dict, env):\n",
    "        \"\"\"\n",
    "        Updates the Q-table after performing an action, using an available action in the new state\n",
    "        :param obs: New state that resulted from a action\n",
    "        :param reward: The reward returned from applying a action to a state\n",
    "        :param done: Boolean representing if the episode is finished\n",
    "        :param info: Additional info from performing an action\n",
    "        :param env: The environment\n",
    "        \"\"\"\n",
    "        if repr(obs) not in self.board_dict:\n",
    "            if not self.board_dict:\n",
    "                self.board_dict[repr(obs)] = 0\n",
    "                self.lastID = 0\n",
    "            else:\n",
    "                self.lastID += 1\n",
    "                self.board_dict[repr(obs)] = self.lastID\n",
    "\n",
    "        state = self.board_dict[repr(info[\"old_state\"])]\n",
    "        new_state = self.board_dict[repr(obs)]\n",
    "        action = info['action']\n",
    "        new_action = self.choice(env)\n",
    "\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * (reward + self.discount_factor * self.Q[new_state, new_action] - self.Q[state, action])\n",
    "\n",
    "    def episode_update(self, episode: int) -> None:\n",
    "        \"\"\"\n",
    "        Update internals after each episode\n",
    "        :param episode: Finished episode id\n",
    "        \"\"\"\n",
    "        # Cutting down on exploration by reducing the epsilon\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay*episode)"
   ]
  },
  {
   "source": [
    "Final game Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "env = NeutreekoEasyEnv(render_mode='terminal')\n",
    "agent = SARSAAgent()\n",
    "\n",
    "# Creating lists to keep track of reward and epsilon values\n",
    "training_rewards = []\n",
    "epsilons = []\n",
    "\n",
    "NB_EPISODES = 1000\n",
    "for episode in range(1, NB_EPISODES + 1):\n",
    "    # Resetting the environment each time as per requirement\n",
    "    env.reset()\n",
    "    # Starting the tracker for the rewards\n",
    "    total_training_rewards = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choice(env)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        agent.update(obs, reward, done, info, env)\n",
    "        total_training_rewards += reward\n",
    "    print(f\"Episode {episode: <4} finished after {env.game.turns_count} turns\")\n",
    "\n",
    "    agent.episode_update(episode)\n",
    "\n",
    "    # Adding the total reward and reduced epsilon values\n",
    "    training_rewards.append(total_training_rewards)\n",
    "    epsilons.append(agent.epsilon)\n",
    "    \n",
    "print(f'Highest board id -> {agent.lastID}')\n",
    "print(f'Q-table -> {agent.Q}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results and total reward over all episodes\n",
    "x = range(NB_EPISODES)\n",
    "plt.plot(x, training_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Training total reward')\n",
    "plt.title('Total rewards over all episodes in training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the epsilons over all episodes\n",
    "plt.plot(epsilons)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title(\"Epsilon for episode\")\n",
    "plt.show()\n"
   ]
  }
 ]
}